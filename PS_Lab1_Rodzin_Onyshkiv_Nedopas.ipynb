{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PS_Lab1_Rodzin_Onyshkiv_Nedopas.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGPIIJlJweUt"
      },
      "source": [
        "# Lab 1 - Naive Bayes Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0GDMQKCJwvIL"
      },
      "source": [
        "## Work breakdown\n",
        "* Taras Rodzin - `fit()` and its helper functions\n",
        "* Taras Onyshkiv - `process_data()` and its helper functions\n",
        "* Olesia Nedopas - `predict_prob()` , `predict()` and `score()`\n",
        "\n",
        "The descriptive contributions to this notebook, function documentation and construction of the algorithm were made jointly"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1O-SSLytW2k"
      },
      "source": [
        "## Introduction\n",
        "During the past three weeks, you learned a couple of essential notions ant theorems. One of them is Bayes theorem.\n",
        "\n",
        "One of its applications is **Naive Bayes classifier**, which is a probabilistic classifier whose aim is to determine which class some observation probably belongs by using the Bayes formula:\n",
        "$$\\mathsf{P}(\\mathrm{class}\\mid \\mathrm{observation})=\\frac{\\mathsf{P}(\\mathrm{observation}\\mid\\mathrm{class})\\mathsf{P}(\\mathrm{class})}{\\mathsf{P}(\\mathrm{observation})}$$\n",
        "\n",
        "Under the strong independence assumption, one can calculate $\\mathsf{P}(\\mathrm{observation} \\mid \\mathrm{class})$ as\n",
        "$$\\mathsf{P}(\\mathrm{observation}) = \\prod_{i=1}^{n} \\mathsf{P}(\\mathrm{feature}_i),$$\n",
        "where $n$ is the total number of features describing a given observation. Thus, $\\mathsf{P}(\\mathrm{class}|\\mathrm{observation})$ now can be calculated as\n",
        "\n",
        "$$\\mathsf{P}(\\mathrm{class} \\mid \\mathrm{\\mathrm{observation}}) = \\mathsf{P}(\\mathrm{class})\\times \\prod_{i=1}^{n}\\frac{\\mathsf{P}(\\mathrm{feature}_i\\mid \\mathrm{class})}{\\mathsf{P}(\\mathrm{feature}_i)}$$\n",
        "\n",
        "For more detailed explanation, you can check [this link](https://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VzV_ykevv9Tp"
      },
      "source": [
        "## Data  description\n",
        "\n",
        "\n",
        "* **sentiment**\n",
        "All the text messages contained in this data set are labeled with three sentiments: positive, neutral or negative. The task is to classify some text message as the one of positive mood, negative or neutral.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASP_dWuj9t0l"
      },
      "source": [
        "##Implementation\n",
        "Processing the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XeSKb6pQ9DoL"
      },
      "source": [
        "import pandas as pd\n",
        "import string\n",
        "\n",
        "\n",
        "def find_stop_words(path: str) -> list:\n",
        "    \"\"\"\n",
        "    Generates stop words to reduce the messages' space\n",
        "    \"\"\"\n",
        "    with open(path, \"r\") as file:\n",
        "        result = [line.replace(\"\\n\", \"\") for line in file]\n",
        "    return result\n",
        "\n",
        "\n",
        "def remove_extra(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Removes all punctuation from message\n",
        "    :param: str - Tito, mutti\n",
        "    :return: str - Tito mutti\n",
        "    \"\"\"\n",
        "    stop_words = find_stop_words(\"/stop_words.txt\")\n",
        "    text_list = text.split(\" \")\n",
        "\n",
        "    for word in text_list:\n",
        "\n",
        "        if word in stop_words:\n",
        "            text_list.remove(word)\n",
        "\n",
        "    text = \" \".join(text_list)\n",
        "\n",
        "    return text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "\n",
        "def gen_box_of_words(message: list) -> dict:\n",
        "    \"\"\"\n",
        "    Convert message to a box of words\n",
        "    :param message: list of words\n",
        "    \"\"\"\n",
        "    word_box = {}\n",
        "\n",
        "    for word in message:\n",
        "\n",
        "        if word not in word_box:\n",
        "            word_box[word] = 1\n",
        "        else:\n",
        "            word_box[word] += 1\n",
        "\n",
        "    return word_box\n",
        "\n",
        "\n",
        "def prepare_message(message: str) -> dict:\n",
        "    \"\"\"\n",
        "    :param message: str - message preparation to be converted\n",
        "    \"\"\"\n",
        "    message = message.lower()\n",
        "    message = remove_extra(message)\n",
        "    message = list(filter(lambda x: x != \"\", message.split(' ')))\n",
        "\n",
        "    return gen_box_of_words(message)\n",
        "\n",
        "\n",
        "def process_data(data_file: str) -> tuple:\n",
        "    \"\"\"\n",
        "    Function for data processing and split it into X and y sets.\n",
        "    :param data_file: str - train datado a research of your own\n",
        "    :return: pd.DataFrame|list, pd.DataFrame|list - X and y data frames or lists\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(data_file)\n",
        "    df['text'] = df['text'].apply(prepare_message)\n",
        "\n",
        "    return (list(df[\"text\"]), list(df[\"sentiment\"]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95OjORbi9zAW"
      },
      "source": [
        "### Implementation\n",
        "The Naive Bayes Classifier \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rx2-IVxy-R99"
      },
      "source": [
        "class BayesianClassifier:\n",
        "    \"\"\"\n",
        "    Implementation of Naive Bayes classification algorithm.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.partition_sizes = None\n",
        "        self.partition_probs = None\n",
        "        self.partition_word_sizes = None\n",
        "        self.messages = None\n",
        "        self.labels = None\n",
        "        self.words = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Fit Naive Bayes parameters according to train data X and y.\n",
        "        :param X: pd.DataFrame|list - train input/messages\n",
        "        :param y: pd.DataFrame|list - train output/labels\n",
        "        :return: None\n",
        "        \"\"\"\n",
        "        self.messages = X\n",
        "        self.labels = y\n",
        "        self.words = calculate_uniques(self.messages)\n",
        "\n",
        "        self.partition_sizes, self.partition_word_sizes = calculate_partitions(\n",
        "            self.labels, self.messages)\n",
        "\n",
        "        self.partition_probs = calculate_partition_probs(\n",
        "            self.partition_sizes, len(self.labels))\n",
        "\n",
        "    def predict_prob(self, message, label):\n",
        "        \"\"\"\n",
        "        Calculate the probability that a given label can be assigned to a given message.\n",
        "        :param message: str - input message\n",
        "        :param label: str - label\n",
        "        :return: float - probability P(label|message)\n",
        "        \"\"\"\n",
        "        prob = self.partition_probs[label]\n",
        "\n",
        "        for word in message:\n",
        "\n",
        "            word_in_partition = word_given_partition(\n",
        "                word, self.messages, self.labels, label)\n",
        "\n",
        "            prob *= word_in_partition / \\\n",
        "                (self.partition_word_sizes[label] + len(self.words))\n",
        "\n",
        "        return prob\n",
        "\n",
        "    def predict(self, message):\n",
        "        \"\"\"\n",
        "        Predict label for a given message.\n",
        "        :param message: str - message\n",
        "        :return: str - label that is most likely to be truly assigned to a given message\n",
        "        \"\"\"\n",
        "        partition_probabilities = {}\n",
        "\n",
        "        for label in self.partition_sizes:\n",
        "            partition_probabilities[self.predict_prob(message, label)] = label\n",
        "\n",
        "        return partition_probabilities[max(partition_probabilities)]\n",
        "\n",
        "    def score(self, X, y):\n",
        "        \"\"\"\n",
        "        Return the mean accuracy on the given test data and labels - the efficiency of a trained model.\n",
        "        :param X: pd.DataFrame|list - test data - messages\n",
        "        :param y: pd.DataFrame|list - test labels\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        score = 0\n",
        "\n",
        "        for idx, message in enumerate(X):\n",
        "            result = self.predict(message)\n",
        "\n",
        "            if result == y[idx]:\n",
        "                score += 1\n",
        "\n",
        "        return score/len(X)\n",
        "\n",
        "\n",
        "def calculate_partitions(labels: list, messages: list) -> dict:\n",
        "    \"\"\"\n",
        "    Return number of messages and words in each partition\n",
        "    :param labels: list of labels\n",
        "    :param messages: list of messages\n",
        "    \"\"\"\n",
        "    sentiments = {\n",
        "        \"positive\": 0,\n",
        "        \"neutral\": 0,\n",
        "        \"negative\": 0\n",
        "    }\n",
        "\n",
        "    words_amount = {\n",
        "        \"positive\": 0,\n",
        "        \"neutral\": 0,\n",
        "        \"negative\": 0\n",
        "    }\n",
        "\n",
        "    for idx, message in enumerate(messages):\n",
        "\n",
        "        sentiments[labels[idx]] += 1\n",
        "        words_amount[labels[idx]] += len(message)\n",
        "\n",
        "    return sentiments, words_amount\n",
        "\n",
        "\n",
        "def calculate_partition_probs(partitions: dict, size: int) -> dict:\n",
        "    \"\"\"\n",
        "    Return probabilities of each partition to occur\n",
        "    :param partitions: dictionary\n",
        "    :param size: int\n",
        "    \"\"\"\n",
        "    partitions_probs = {}\n",
        "\n",
        "    for partition in partitions:\n",
        "        partitions_probs[partition] = partitions[partition] / size\n",
        "\n",
        "    return partitions_probs\n",
        "\n",
        "\n",
        "def word_given_partition(word: str, messages: list, labels: list, label: str) -> int:\n",
        "    \"\"\"\n",
        "    Return number of how many times word occurs in partition\n",
        "    :param word: str\n",
        "    :param messages: list\n",
        "    :param labels: list of labels\n",
        "    :param label: str\n",
        "    \"\"\"\n",
        "    word_partition = 1\n",
        "\n",
        "    for idx, message in enumerate(messages):\n",
        "        if word in message and labels[idx] == label:\n",
        "\n",
        "            word_partition += 1\n",
        "\n",
        "    return word_partition\n",
        "\n",
        "\n",
        "def calculate_uniques(messages: list) -> list:\n",
        "    \"\"\"\n",
        "    Return list of unique words\n",
        "    :param messages: list of messages\n",
        "    \"\"\"\n",
        "    uniques = []\n",
        "\n",
        "    for message in messages:\n",
        "        for word in message:\n",
        "            if word not in uniques:\n",
        "                uniques.append(word)\n",
        "\n",
        "    return uniques"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GW85An5p-sp3"
      },
      "source": [
        "### Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zo4TZVh4950E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a847335b-bba0-4a44-9d00-58cef0958e39"
      },
      "source": [
        "train_X, train_y = process_data(\"/train.csv\")\n",
        "test_X, test_y = process_data(\"/test.csv\")\n",
        "\n",
        "classifier = BayesianClassifier()\n",
        "classifier.fit(train_X, train_y)\n",
        "classifier.predict_prob(test_X[0], test_y[0])\n",
        "\n",
        "print(\"model score: \", classifier.score(test_X, test_y))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model score:  0.3223234624145786\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkNytMyM-8n0"
      },
      "source": [
        "## Conclusions\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IpEbPLoyBTBA"
      },
      "source": [
        "### <b>The method implemented in general:</b>\n",
        "\n",
        "In this assignment we were tasked with implementing a model, which would predict the sentiment of a sentence, based on the analysis of the training data given to us. These type of problems have to do with conditional probability, so one of the ways to solve them is using the Naive Bayes classifier, which is based on the Bayes theorem. <br> \n",
        "Predicting the sentiments of sentences is basically classifying them into 3 categories. To do this, we need to compare the probabilities of a sentence belonging to each of these 3 classes and choosing the most likely one. Using the classifier we calculated the conditional probability that one of the classes can be applied to a sentence by splitting it into words (separate features) and multiplying the probabiities of each of them appearing in a certain class (multiplying this product by the probability of getting the class itself). By analyzing the training data we trained the model to predict, which sentiment, or label, to asssign to a new sentence. <br>\n",
        "When testing the model on unknown examples, though, there may be some words which are absent in the \"bag of words\" (the distribution of instances of all words in the training data), in which case the classifier wil assign a probability of 0 to the word, making the sentence impossible to classify. To prevent this, there's a technique called \"smoothing\", which implies adding 1 to the total instances of every word a priori, and then adding the number of words in the \"bag\" to the denominator of the probability fraction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAivLQzK_Ksa"
      },
      "source": [
        "### <b>Pros and cons of the method:</b>\n",
        "\n",
        "#### Pros\n",
        "* Works fast and efficiently, assuming that the features are independent\n",
        "* Can solve problems that require classifying features into multiple categories\n",
        "* Requires less training data than other models to make predictions about the test data\n",
        "* Easy to implement\n",
        "\n",
        "#### Cons\n",
        "* If the test data contains a feature that was absent in the training data, the classifier will not be able to make any predictions about it, because it will assign 0 to the feature\n",
        "* To make predictions easier it makes an assumption about the features being independent, which may be false in reality, therefore making predictions less accurate. (this is what makes the classifier \"naive\")\n",
        "* Suppose our model is trained on data in which the number of elements of a certain partition is much smaller than the number of elements of other partitions. In that case, this model may give the wrong result for sentences from this partition."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IW1pCL-ZBfUD"
      },
      "source": [
        "### <b>A few sentences about our implementation of the classifier:</b>\n",
        "* In our implementation of this model on the python, we present all messages as bags of words, which simplifies our further work. We do not represent word bags as a list of ones and zeros, but as dictionaries with elements.\n",
        "*in the \"fit ()\" function we generate all the necessary parameters that we will use when searching for probabilities.\n",
        "*Using the function \"predict_prob ()\" we find the probability that this message belongs to a certain sentiment by the formula:</br>\n",
        "        P (sentiment | Message) = P (sentiment) * P (word_1 | sentiment) * (...) * P (word_n | sentiment)\n",
        "  But to avoid cases where the probability of a word is zero, we initially assume that each word was in at least one message from a given mood. Through we divide the number of cases when you found the word in the message not only on the number of messages but on \"the number of messages + the number of unique words.\"\n",
        "*In the \"predict ()\" function, we output the sentiment that is most likely for this message. To do this, we find the probability that this sentence belongs to each of the sentiments and return the sentiment that is most likely.\n",
        "*To increase the accuracy of our model, we remove all punctuation marks and stop words."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZlIcMe8uBz-2"
      },
      "source": [
        "###<b>Describe your results:</b>\n",
        "If we create a model and run the fit () function based on train.csv and execute the score () function on the data from the test.csv file, we can see that our accuracy is approximately 0.32. From this, we could conclude that we did something wrong. But the problem is not in our implementation of this model but in the databases we use. The problem is that in \"train.csv\" only 116 of 3968 messages are negative, so the model shows that the probability that the sentence is negative is very small. And in our \"test.csv\" most of the sentences are negative and our model predicts most of them incorrectly.\n"
      ]
    }
  ]
}